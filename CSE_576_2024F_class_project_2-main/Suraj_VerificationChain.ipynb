{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa250d7e-2575-4808-a60e-4472b6c0eea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33419f0bac80438f8a5afccc97cb46d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    use_cache=None,\n",
    "    attn_implementation=None,\n",
    ")\n",
    "model.device\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f1e6f-c222-4e3c-80aa-71d6636c4252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "def generate_output(prompt, max_tokens=800):\n",
    "    batch = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            temperature=0,\n",
    "            use_cache=True,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,\n",
    "            length_penalty=1,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    output_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    return output_text[len(prompt):].strip()\n",
    "# Load input JSON file\n",
    "input_file = \"legal_reasoning_with_answers.json\"  # Path to input JSON file\n",
    "output_json_file = \"/home/smanylal/MidTermReport1/NLPFinal/New/legal_reasoning_with_questions.json\"  # Path to save the output CSV file\n",
    "try:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading input file: {e}\")\n",
    "    data = {\"legal_scenarios\": []}\n",
    "# Prepare a list to store rows for the CSV\n",
    "output_scenarios = []\n",
    "# Process each scenario\n",
    "for scenario in data.get(\"legal_scenarios\", []):\n",
    "    #scenario_id = scenario['id']\n",
    "    legal_context = scenario['context']\n",
    "    question = scenario['question']\n",
    "    options = scenario['options']\n",
    "    legal_reasoning = scenario['legal_reasoning']\n",
    "    correct_answer = scenario['correct_answer']\n",
    "    \n",
    "    # Generate the verification questions\n",
    "    user_prompt2 = legal_context + question + options + legal_reasoning + '''Task:\n",
    "    You are a expert legal critique whose is given:\n",
    "    1) a given legal_context.\n",
    "    2) a given question.\n",
    "    3) answer options to the given question out of which only one is true. \n",
    "    3) the legal_reasoning behind the legal_context.\n",
    "    \n",
    "    Your task is to thoroughly examine the given reasoning provided and identify any possible faults, fallacies, or loopholes in these legal_reasoning steps. Assume that every legal_reasoning provided contains errors. \n",
    "    Generate compelling questions (atmost 15) aimed at rectifying these error prone reasoning. Start the sentence of each question indicating which Legal_reasoning step they are finding the fault of. For eg: In Step n: 'followed by the questioning the fault in step n.'\n",
    "\n",
    "    Important: ONLY the QUESTIONS are expected from you.\n",
    "    Please ensure that the output is clearly separated by special markers so that it can be parsed into individual elements later on.\n",
    "\n",
    "    Follow this format:\n",
    "    --1 In Step n: [question]\n",
    "    --2 In Step n+1: [question]\n",
    "    --3 ...\n",
    "    --n \n",
    "    '''\n",
    "    \n",
    "    verification_questions_raw = generate_output(user_prompt2)\n",
    "    \n",
    "    # Parse verification questions into a list by splitting using markers (--1, --2, etc.)\n",
    "    questions_start = verification_questions_raw.find(\"--1\")\n",
    "    if questions_start != -1:\n",
    "        questions_text = verification_questions_raw[questions_start:].strip()\n",
    "        # Split by the markers '--1', '--2', etc. and filter empty parts\n",
    "        questions_list = [q.strip() for q in questions_text.split('\\n') if q.startswith(\"--\")]\n",
    "        \n",
    "        # Only keep the first 15 questions\n",
    "        verification_questions = questions_list[:15]\n",
    "    else:\n",
    "        verification_questions = [\"Error parsing verification questions.\"]\n",
    "    \n",
    "    # Add to output scenarios\n",
    "    output_scenarios.append({\n",
    "        #\"id\": scenario_id,\n",
    "        \"context\": legal_context,\n",
    "        \"question\": question,\n",
    "        \"options\": options,\n",
    "        \"legal_reasoning\": legal_reasoning,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"verification_questions\": verification_questions\n",
    "    })\n",
    "\n",
    "    # Wrap in the required structure\n",
    "    output_data = {\n",
    "        \"legal_scenarios\": output_scenarios\n",
    "    }\n",
    "    \n",
    "    # Save to JSON file\n",
    "    try:\n",
    "        with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "        #print(f\"Output saved to {output_json_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving output file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133648dc-23b4-452b-9cd4-7770abd5bce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
