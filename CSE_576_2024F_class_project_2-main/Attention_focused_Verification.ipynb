{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6b1c0e-3186-4d48-99c4-915d4f3b49af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f376a6964842d7a491a1a73d0ca7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    use_cache=None,\n",
    "    attn_implementation=None,\n",
    ")\n",
    "model.device\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7a4aa7-1851-4bbf-bfe2-4bdc28a4c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmadhav6/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Function to generate model responses\n",
    "def generate_output(prompt, max_tokens=1000):\n",
    "    batch = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            temperature=0,\n",
    "            use_cache=True,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,\n",
    "            length_penalty=1,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    output_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    return output_text[len(prompt):].strip()\n",
    "\n",
    "# Load input CSV\n",
    "input_file = \"legal_reasoning_30.csv\"  # Path to input file\n",
    "output_file = \"/home/rmadhav6/legal_reasoning_with_answers.csv\"  # Path to save the output\n",
    "try:\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(input_file, encoding='ISO-8859-1')\n",
    "\n",
    "# Initialize empty lists to store outputs\n",
    "output_text1_list = []\n",
    "output_text2_list = []\n",
    "output_text3_list = []\n",
    "\n",
    "# Process each row\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        legal_context = row['Context']  # Assuming these are the column names\n",
    "        question = row['Question']\n",
    "        options = row['Options']\n",
    "        analysis = row['Analysis']\n",
    "        \n",
    "        # Generate first output\n",
    "        user_prompt1 = legal_context + question + options + analysis + '''Task:\n",
    "        You are a helpful legal assistant. Choose the correct option by performing legal reasoning while strictly adhering to \n",
    "        the legal context and analysis provided.\n",
    "        While answering make sure to use the following format:\n",
    "        [explanation of your legal reasoning step by step as numbered points]'''\n",
    "        output_text1 = generate_output(user_prompt1)\n",
    "        output_text1_list.append(output_text1)\n",
    "\n",
    "        # Generate second output\n",
    "        user_prompt2 = legal_context + question + options + analysis + output_text1 + '''Task:\n",
    "        You are a helpful legal assistant. \n",
    "        You need to identify critical terms from the legal reasoning steps based on the legal context, question, options and analysis.\n",
    "        You need to generate verification questions for each of the legal reasoning steps and these questions must focus on the critical terms identified.'''\n",
    "        output_text2 = generate_output(user_prompt2)\n",
    "        output_text2_list.append(output_text2)\n",
    "\n",
    "        # Generate third output\n",
    "        #user_prompt3 = legal_context + question + options + output_text1 + output_text2 + ''' Task:\n",
    "        #You are a helpful legal assistant. You need to identify the correct answer to the question.\n",
    "        #Analyze the verification questions and their answers for each of the legal reasoning steps and based on that modify \n",
    "        #the legal reasoning and give the correct option to the question.'''\n",
    "        user_prompt3 = legal_context + question + options + analysis + output_text1 + output_text2 + '''Task:\n",
    "        You are a helpful legal assistant. You need to identify the correct answer to the question.\n",
    "        Based on the answers to the verification questions, label each of the legal reasoning steps as correct or wrong.\n",
    "        Then based on the evaluation of the legal reasoning steps, perform legal reasoning again adhering to the legal context \n",
    "        and analysis and find the correct option to the question.'''\n",
    "        output_text3 = generate_output(user_prompt3, max_tokens=1500)\n",
    "        output_text3_list.append(output_text3)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully and log them\n",
    "        output_text1_list.append(f\"Error: {str(e)}\")\n",
    "        output_text2_list.append(f\"Error: {str(e)}\")\n",
    "        output_text3_list.append(f\"Error: {str(e)}\")\n",
    "\n",
    "# Add the outputs to the DataFrame\n",
    "df['output_text1'] = output_text1_list\n",
    "df['output_text2'] = output_text2_list\n",
    "df['output_text3'] = output_text3_list\n",
    "\n",
    "# Save the updated DataFrame to a CSV file\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bd9e0-d533-49df-b0dd-2497fb16080c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
