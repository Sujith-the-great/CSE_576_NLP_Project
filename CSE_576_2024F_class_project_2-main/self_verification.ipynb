{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6b1c0e-3186-4d48-99c4-915d4f3b49af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291b2d1a8cee4f11a5b416bf86b24699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    use_cache=None,\n",
    "    attn_implementation=None,\n",
    ")\n",
    "model.device\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193bd9e0-d533-49df-b0dd-2497fb16080c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apai14/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to /home/apai14/NLP/legal_reasoning_with_answers.csv\n"
     ]
    }
   ],
   "source": [
    "def generate_output(prompt, max_tokens=1000):\n",
    "    batch = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **batch,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            temperature=0,\n",
    "            use_cache=True,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.2,\n",
    "            length_penalty=1,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    output_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "    return output_text[len(prompt):].strip()\n",
    "# Load input JSON file\n",
    "input_file = \"legal_reasoning_30.json\"  # Path to input JSON file\n",
    "output_csv_file = \"/home/apai14/NLP/legal_reasoning_with_answers.csv\"  # Path to save the output CSV file\n",
    "try:\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading input file: {e}\")\n",
    "    data = {\"legal_scenarios\": []}\n",
    "# Prepare a list to store rows for the CSV\n",
    "csv_rows = []\n",
    "# Process each scenario\n",
    "for scenario in data.get(\"legal_scenarios\", []):\n",
    "    try:\n",
    "        scenario_id = scenario['id']\n",
    "        legal_context = scenario['context']\n",
    "        question = scenario['question']\n",
    "        options = scenario['options']\n",
    "        analysis = scenario.get('ground_truth', '')  # Using 'ground_truth' as a substitute for 'analysis'\n",
    "        # Generate first output\n",
    "        user_prompt1 = legal_context + question + options + analysis + '''Task:\n",
    "        You are a helpful legal assistant. Choose the correct option by performing legal reasoning while strictly adhering to\n",
    "        the legal context and analysis provided.\n",
    "        While answering make sure to use the following format:\n",
    "        [explanation of your legal reasoning step by step as numbered points]'''\n",
    "        output_text1 = generate_output(user_prompt1)\n",
    "        scenario['output_text1'] = output_text1\n",
    "        # Generate second output\n",
    "        user_prompt2 = legal_context + question + options + analysis + output_text1 + '''Task:\n",
    "        You are a helpful legal assistant.\n",
    "        You need to generate verification questions for each of the legal reasoning steps based on the legal context, question, options and analysis.\n",
    "        Based on the verification questions and answers give feedback for the legal reasoning steps and analyse it to find the correct option to the question.'''\n",
    "        output_text2 = generate_output(user_prompt2)\n",
    "        scenario['output_text2'] = output_text2\n",
    "        # Append the data to the list for CSV\n",
    "        csv_rows.append({\n",
    "            \"ID\": scenario_id,\n",
    "            \"Context\": legal_context,\n",
    "            \"Question\": question,\n",
    "            \"Options\": options,\n",
    "            \"Ground Truth\": analysis,\n",
    "            \"Output Text 1\": output_text1,\n",
    "            \"Output Text 2\": output_text2,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully and log them\n",
    "        csv_rows.append({\n",
    "            \"ID\": scenario.get('id', 'N/A'),\n",
    "            \"Context\": scenario.get('context', 'N/A'),\n",
    "            \"Question\": scenario.get('question', 'N/A'),\n",
    "            \"Options\": scenario.get('options', 'N/A'),\n",
    "            \"Ground Truth\": scenario.get('ground_truth', 'N/A'),\n",
    "            \"Output Text 1\": f\"Error: {str(e)}\",\n",
    "            \"Output Text 2\": f\"Error: {str(e)}\"\n",
    "        })\n",
    "# Convert the list of rows into a DataFrame\n",
    "df = pd.DataFrame(csv_rows)\n",
    "# Save the DataFrame to a CSV file\n",
    "try:\n",
    "    df.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "    print(f\"Output saved to {output_csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving output file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f0c5a-9d22-464b-b6b1-0adb6d49392b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
